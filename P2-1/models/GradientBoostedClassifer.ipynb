{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fbd328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.metrics as skm\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8a539c",
   "metadata": {},
   "source": [
    "# General setup and helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7fc9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/preprocessed/reviewFeatures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52952e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Load target features on y and drop target features on X.\n",
    "  Perform shuffle on first split to prevent bias. Split 80-20 for train-test.\n",
    "  Split again on train to get validation set. So 60-20-20.\n",
    "  Stratifies all subsets to ensure equal samples for each class.\n",
    "\"\"\"\n",
    "def train_test_valid_split(df, targets):\n",
    "  y = df[targets]\n",
    "  X = df.drop(targets, axis=1)\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=101, shuffle=True, stratify=y)\n",
    "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state=101, shuffle=True, stratify=y_train)\n",
    "\n",
    "  return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2250497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Merges training and validation data.\n",
    "\"\"\"\n",
    "def concat_train_valid(X_train, y_train, X_val, y_val):\n",
    "  X_train = pd.concat([X_train, X_val])\n",
    "  y_train = pd.concat([y_train, y_val])\n",
    "\n",
    "  return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6130ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Saves the best model as joblib file.\n",
    "\"\"\"\n",
    "def save_model(model):\n",
    "    best_model = model.best_estimator_\n",
    "\n",
    "    model_fp = os.path.join(\"../models\", \"best_gbc_model.joblib\")\n",
    "\n",
    "    joblib.dump(best_model, model_fp)\n",
    "\n",
    "    print(f\"Saved best Gradient-Boosted Tree Classifer to {model_fp}\")\n",
    "\n",
    "    return best_model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1bee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Prints the classification report for training, test, and validation only if hold-out was used.\n",
    "    Also prints out the most important features and the number of features retained after feature selection if grid search was used.\n",
    "\"\"\"\n",
    "def printResults(classifer, X_train, X_test, X_val, y_train, y_test, y_val, grid_search=False, param_grid=None):\n",
    "    train_pred = classifer.predict(X_train)\n",
    "    print(f'Training Report')\n",
    "    print(classification_report(y_train, train_pred))\n",
    "\n",
    "    train_report = classification_report(y_train, train_pred, output_dict=True)\n",
    "\n",
    "    print(f\"Training Accuracy {train_report['accuracy']:.4f}\")\n",
    "    print(f\"Training F1-Score: {train_report['macro avg']['f1-score']:.4f}\\n\")  \n",
    "\n",
    "    if grid_search == False:\n",
    "        val_pred = classifer.predict(X_val)    \n",
    "        print(f'Validation Report')\n",
    "        print(classification_report(y_val, val_pred))\n",
    "\n",
    "        val_report = classification_report(y_test, val_pred, output_dict=True)\n",
    "        print(f\"Validation Accuracy {val_report['accuracy']:.4f}\")\n",
    "        print(f\"Validation F1-Score: {val_report['macro avg']['f1-score']:.4f}\\n\")    \n",
    "\n",
    "    test_pred = classifer.predict(X_test)\n",
    "    print(f'Test Report')\n",
    "    print(classification_report(y_test, test_pred))\n",
    "\n",
    "    test_report = classification_report(y_test, test_pred, output_dict=True)\n",
    "    print(f\"Test Accuracy {test_report['accuracy']:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_report['macro avg']['f1-score']:.4f}\\n\")      \n",
    "\n",
    "    if grid_search == True and param_grid is not None:\n",
    "        fs = classifer.named_steps['fs']\n",
    "        feature_retained = fs.get_support()\n",
    "        feature_names = X_train.columns[feature_retained]\n",
    "\n",
    "        clf = classifer.named_steps['classifer']\n",
    "\n",
    "        most_important_features = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': clf.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        print(f'Number of features retained: {len(feature_names)}')\n",
    "\n",
    "        print(\"Most Important Features: \")\n",
    "        print(most_important_features.head(10))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b0b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Trains a Gradient Boosted ensemble model by training weak decision trees to produce residuals to reduce the residuals of the main strong model.\n",
    "    First, performs a train-test-valid split of 60-20-20. Then performs feature selection using feature importance and a threshold value. \n",
    "    Features with absolute importance values less than the threshold value are discarded, while the rest are retained.\n",
    "    If grid search flag is true and a parameter grid has been specified, then merge the train and validation subsets and run grid search.\n",
    "    Then train the classifer and the results gets printed out via the printResults function.\n",
    "    Additionally, provided grid search is enabled, print the best model parameters and score and save the best model.\n",
    "\"\"\"\n",
    "def GBClassification(df, grid_search=False, param_grid=None):\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = train_test_valid_split(df, \"Real=1/Fake=0\")\n",
    "\n",
    "    # print(\"Training set distribution: \")\n",
    "    # print(f'{y_train.value_counts()}')\n",
    "\n",
    "    # print(\"Validation set distribution: \")\n",
    "    # print(f'{y_val.value_counts()}')       \n",
    "\n",
    "    # print(\"Test set distribution: \")\n",
    "    # print(f'{y_test.value_counts()}')     \n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('fs', SelectFromModel(estimator=GradientBoostingClassifier(random_state=101, learning_rate=0.01, max_features='sqrt', subsample=0.1))), \n",
    "        ('classifer', GradientBoostingClassifier(random_state=101, subsample=0.1))\n",
    "    ])\n",
    "\n",
    "    if grid_search == True and param_grid is not None:\n",
    "        X_train, y_train = concat_train_valid(X_train, y_train, X_val, y_val)\n",
    "\n",
    "        # print(\"Merged Training set distribution: \")\n",
    "        # print(f'{y_train.value_counts()}')\n",
    "\n",
    "        pipeline = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, scoring=\"f1\")\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if grid_search == True and param_grid is not None:\n",
    "        print(f'Best Model Parameters: {pipeline.best_params_}')\n",
    "        print(f'Best Model Score: {pipeline.best_score_}')\n",
    "\n",
    "        pipeline = save_model(pipeline)\n",
    "\n",
    "    printResults(pipeline, X_train, X_test, X_val, y_train, y_test, y_val, grid_search=grid_search, param_grid=param_grid)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27cff943",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'fs__threshold': ['mean', 'median', '1.25*mean'],\n",
    "    'classifer__n_estimators': [50, 100, 200],\n",
    "    'classifer__learning_rate': [0.001, 0.01, 0.1],\n",
    "    'classifer__max_depth': [3, 5, 7],\n",
    "    'classifer__min_samples_split': [5, 10, 20],\n",
    "    'classifer__min_samples_leaf': [2, 4, 10],\n",
    "    'classifer__subsample': [0.1, 0.4, 0.5, 0.8],\n",
    "    'classifer__max_features': [None, 'sqrt']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2babbb5d",
   "metadata": {},
   "source": [
    "# Hold-Out Method (Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d795518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.59      0.67        78\n",
      "           1       0.67      0.83      0.74        78\n",
      "\n",
      "    accuracy                           0.71       156\n",
      "   macro avg       0.72      0.71      0.71       156\n",
      "weighted avg       0.72      0.71      0.71       156\n",
      "\n",
      "Training Accuracy 0.7115\n",
      "Training F1-Score: 0.7072\n",
      "\n",
      "Validation Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.62      0.64        26\n",
      "           1       0.64      0.69      0.67        26\n",
      "\n",
      "    accuracy                           0.65        52\n",
      "   macro avg       0.65      0.65      0.65        52\n",
      "weighted avg       0.65      0.65      0.65        52\n",
      "\n",
      "Validation Accuracy 0.5769\n",
      "Validation F1-Score: 0.5763\n",
      "\n",
      "Test Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.70        26\n",
      "           1       0.71      0.58      0.64        26\n",
      "\n",
      "    accuracy                           0.67        52\n",
      "   macro avg       0.68      0.67      0.67        52\n",
      "weighted avg       0.68      0.67      0.67        52\n",
      "\n",
      "Test Accuracy 0.6731\n",
      "Test F1-Score: 0.6700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GBClassification(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430ea6d",
   "metadata": {},
   "source": [
    "# 5-fold CV (Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f6f7a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Parameters: {'classifer__learning_rate': 0.1, 'classifer__max_depth': 3, 'classifer__max_features': 'sqrt', 'classifer__min_samples_leaf': 2, 'classifer__min_samples_split': 10, 'classifer__n_estimators': 50, 'classifer__subsample': 0.5, 'fs__threshold': 'median'}\n",
      "Best Model Score: 0.6846819546819546\n",
      "Saved best Gradient-Boosted Tree Classifer to ../models/best_gbc_model.joblib\n",
      "Training Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80       104\n",
      "           1       0.82      0.75      0.78       104\n",
      "\n",
      "    accuracy                           0.79       208\n",
      "   macro avg       0.80      0.79      0.79       208\n",
      "weighted avg       0.80      0.79      0.79       208\n",
      "\n",
      "Training Accuracy 0.7933\n",
      "Training F1-Score: 0.7929\n",
      "\n",
      "Test Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.81      0.72        26\n",
      "           1       0.75      0.58      0.65        26\n",
      "\n",
      "    accuracy                           0.69        52\n",
      "   macro avg       0.70      0.69      0.69        52\n",
      "weighted avg       0.70      0.69      0.69        52\n",
      "\n",
      "Test Accuracy 0.6923\n",
      "Test F1-Score: 0.6882\n",
      "\n",
      "Number of features retained: 5\n",
      "Most Important Features: \n",
      "  Feature  Importance\n",
      "2     NOW    0.290568\n",
      "0     AWL    0.230597\n",
      "4     NST    0.168933\n",
      "1     ASL    0.162006\n",
      "3     NAJ    0.147895\n"
     ]
    }
   ],
   "source": [
    "GBClassification(df, grid_search=True, param_grid=param_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
