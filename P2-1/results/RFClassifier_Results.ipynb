{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This random forest classification model was trained using a combination of 5-fold cross validation, hyperparameter tuning, and feature selection. First the data set was split in a 60-20-20 training, validation, and testing ratio. Each split was stratified to ensure an equal number of fake and real reviews in each subset. Each subset was also shuffled. \n",
    "\n",
    "Grid search is performed using 5-fold cross validation and hyperparameter tuning. The best model is found according to f1 score.\n",
    "\n",
    "The model is first trained without feature selection to measure the base performance. Then the process is repeated using recursive feature elimination with cross validation. This process tests different feature subsets until it finds the optimal feature set. For every number of features, RFECV eliminates the weakest feature and trains a model on this new feature set to measure performance. This model is trained on k folds to ensure consistency. After finding the ideal number of features, as well as the specific subset of features, it returns this optimal feature set. After extracting the optimal feature set, it is then used to train a model using grid search. After grid search finds the best model, we then save the model using the joblib library. Train and test metrics are also recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two models were trained for this dataset. One model was trained without feature selection, and one was trained with feature selection.\n",
    "\n",
    "The model trained without feature selection achieved a training accuracy of 0.7981 and a training f1 score of 0.7766. When evaluating on the test set, the model achieved a test accuracy of 0.6731 and a test f1 score of 0.6222. Considering these metrics, the model without feature selection appears to be overfitting the dataset. With random forest, having extra noisy features can affect the learning of the model. These noisy features lead the random forest to make decisions based on the bad data that is only present in the training set. The hyperparameters of the best model are: 'max_depth': 4, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, and 'n_estimators': 500.\n",
    "\n",
    "The model trained with feature selection acheived a training accuracy of 0.7452 and a training f1 score of 0.7104. When evaluating on the test set, the model achieved a test accuracy of 0.6923 and a test f1 score of 0.6522. With these metrics, the model shows less signs of overfitting and is slightly more generalized. The lower training scores compared to the model without feature selection shows that the feature selected model is not memorizing the dataset to the same extent. This allows it to be better at generalizing, leading to better test metrics. This is likely because of the reduction of the amount of features. The hyperparameters of the best model are: 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 200. For this model, 7 features were selected: 'AWL', 'ASL', 'NOW', 'NVB', 'NAJ', 'NST', and 'CDV'. The most important feature in this set is NAJ(number of adjectives). "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
